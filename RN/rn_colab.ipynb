{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuiYnX_uI0uM"
      },
      "source": [
        "# TensorFlow e tf.keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "# Bibliotecas de ajuda\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "\n",
        "data = pd.read_csv(\"iris.data\", header=None)\n",
        "dataset = data.values\n",
        "X = dataset[:,0:4].astype(float)\n",
        "Y = dataset[:,4]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDXKyYKMJhNN"
      },
      "source": [
        "'''\n",
        "Separar o tipo ‘Iris-setosa’, ‘Iris-versicolor’, ‘Iris-virginica’ em um array binario para identificar o tipo.\n",
        "\n",
        "'''\n",
        "Y = pd.get_dummies(Y)\n",
        "Y = Y.values\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zlwvzid5KAFB"
      },
      "source": [
        "#Agora, vamos dividir o conjunto de dados em conjunto de treino e teste. Neste caso, utilizaremos 80% do dataset em treino e 20% em teste\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=1)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pB5NiBoXKcBf"
      },
      "source": [
        "# pré-processamento dos dados\n",
        "scaler = preprocessing.StandardScaler().fit(X_train)\n",
        "scaler.transform(X_train)\n",
        "scaler.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-rfAMgAKvGN"
      },
      "source": [
        "def create_model(X_train, X_test, y_train, y_test):\n",
        "    model = Sequential()    \n",
        "    model.add(Dense(16, input_dim=4)) #primeira camada (input)\n",
        "    model.add(Activation('relu'))     #primeira camada - func. ativ.\n",
        "    model.add(Dense(16))              #camada intermediária\n",
        "    model.add(Activation('relu'))     #camada intermediária - func. ativ.\n",
        "    model.add(Dense(3))               #última camada (output)\n",
        "    model.add(Activation('softmax'))  #última camada - func. ativ.\n",
        "    # Compile model\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam',     metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8foUugvtLXEz",
        "outputId": "8654e37b-d80a-484c-bda3-403db7d631f1"
      },
      "source": [
        "model = create_model(X_train, y_train, X_test, y_test)\n",
        "name = \"{}\".format(int(time.time()))\n",
        "tensorboard = TensorBoard(log_dir='tensorboard/{}'.format(name))\n",
        "model.fit(X_train, y_train, epochs=200, verbose=2, \n",
        "                 validation_data=(X_test, y_test),callbacks=[tensorboard])\n",
        "model.summary() "
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "4/4 - 1s - loss: 1.5665 - accuracy: 0.3083 - val_loss: 1.4465 - val_accuracy: 0.4333\n",
            "Epoch 2/200\n",
            "4/4 - 0s - loss: 1.4481 - accuracy: 0.3083 - val_loss: 1.3779 - val_accuracy: 0.4333\n",
            "Epoch 3/200\n",
            "4/4 - 0s - loss: 1.3517 - accuracy: 0.3083 - val_loss: 1.3199 - val_accuracy: 0.4333\n",
            "Epoch 4/200\n",
            "4/4 - 0s - loss: 1.2846 - accuracy: 0.3083 - val_loss: 1.2741 - val_accuracy: 0.4333\n",
            "Epoch 5/200\n",
            "4/4 - 0s - loss: 1.2200 - accuracy: 0.3083 - val_loss: 1.2303 - val_accuracy: 0.4333\n",
            "Epoch 6/200\n",
            "4/4 - 0s - loss: 1.1732 - accuracy: 0.3083 - val_loss: 1.1823 - val_accuracy: 0.4333\n",
            "Epoch 7/200\n",
            "4/4 - 0s - loss: 1.1259 - accuracy: 0.3083 - val_loss: 1.1371 - val_accuracy: 0.4333\n",
            "Epoch 8/200\n",
            "4/4 - 0s - loss: 1.0816 - accuracy: 0.3083 - val_loss: 1.0995 - val_accuracy: 0.4333\n",
            "Epoch 9/200\n",
            "4/4 - 0s - loss: 1.0475 - accuracy: 0.3500 - val_loss: 1.0638 - val_accuracy: 0.4333\n",
            "Epoch 10/200\n",
            "4/4 - 0s - loss: 1.0149 - accuracy: 0.4500 - val_loss: 1.0299 - val_accuracy: 0.4667\n",
            "Epoch 11/200\n",
            "4/4 - 0s - loss: 0.9841 - accuracy: 0.4833 - val_loss: 1.0013 - val_accuracy: 0.5333\n",
            "Epoch 12/200\n",
            "4/4 - 0s - loss: 0.9542 - accuracy: 0.5250 - val_loss: 0.9763 - val_accuracy: 0.5000\n",
            "Epoch 13/200\n",
            "4/4 - 0s - loss: 0.9321 - accuracy: 0.5333 - val_loss: 0.9563 - val_accuracy: 0.4667\n",
            "Epoch 14/200\n",
            "4/4 - 0s - loss: 0.9082 - accuracy: 0.5583 - val_loss: 0.9398 - val_accuracy: 0.4000\n",
            "Epoch 15/200\n",
            "4/4 - 0s - loss: 0.8877 - accuracy: 0.5500 - val_loss: 0.9252 - val_accuracy: 0.3333\n",
            "Epoch 16/200\n",
            "4/4 - 0s - loss: 0.8642 - accuracy: 0.5083 - val_loss: 0.9105 - val_accuracy: 0.3333\n",
            "Epoch 17/200\n",
            "4/4 - 0s - loss: 0.8447 - accuracy: 0.5000 - val_loss: 0.8981 - val_accuracy: 0.3000\n",
            "Epoch 18/200\n",
            "4/4 - 0s - loss: 0.8289 - accuracy: 0.5500 - val_loss: 0.8899 - val_accuracy: 0.4333\n",
            "Epoch 19/200\n",
            "4/4 - 0s - loss: 0.8131 - accuracy: 0.7000 - val_loss: 0.8761 - val_accuracy: 0.4667\n",
            "Epoch 20/200\n",
            "4/4 - 0s - loss: 0.7994 - accuracy: 0.7417 - val_loss: 0.8613 - val_accuracy: 0.5333\n",
            "Epoch 21/200\n",
            "4/4 - 0s - loss: 0.7864 - accuracy: 0.7833 - val_loss: 0.8468 - val_accuracy: 0.5667\n",
            "Epoch 22/200\n",
            "4/4 - 0s - loss: 0.7721 - accuracy: 0.7917 - val_loss: 0.8347 - val_accuracy: 0.6000\n",
            "Epoch 23/200\n",
            "4/4 - 0s - loss: 0.7576 - accuracy: 0.7917 - val_loss: 0.8223 - val_accuracy: 0.6000\n",
            "Epoch 24/200\n",
            "4/4 - 0s - loss: 0.7444 - accuracy: 0.7833 - val_loss: 0.8102 - val_accuracy: 0.6000\n",
            "Epoch 25/200\n",
            "4/4 - 0s - loss: 0.7301 - accuracy: 0.7833 - val_loss: 0.7955 - val_accuracy: 0.6000\n",
            "Epoch 26/200\n",
            "4/4 - 0s - loss: 0.7180 - accuracy: 0.7750 - val_loss: 0.7836 - val_accuracy: 0.6000\n",
            "Epoch 27/200\n",
            "4/4 - 0s - loss: 0.7040 - accuracy: 0.7750 - val_loss: 0.7699 - val_accuracy: 0.6000\n",
            "Epoch 28/200\n",
            "4/4 - 0s - loss: 0.6918 - accuracy: 0.7917 - val_loss: 0.7568 - val_accuracy: 0.6000\n",
            "Epoch 29/200\n",
            "4/4 - 0s - loss: 0.6785 - accuracy: 0.8250 - val_loss: 0.7397 - val_accuracy: 0.6667\n",
            "Epoch 30/200\n",
            "4/4 - 0s - loss: 0.6654 - accuracy: 0.8333 - val_loss: 0.7250 - val_accuracy: 0.7000\n",
            "Epoch 31/200\n",
            "4/4 - 0s - loss: 0.6528 - accuracy: 0.8417 - val_loss: 0.7099 - val_accuracy: 0.7000\n",
            "Epoch 32/200\n",
            "4/4 - 0s - loss: 0.6403 - accuracy: 0.8417 - val_loss: 0.6938 - val_accuracy: 0.7000\n",
            "Epoch 33/200\n",
            "4/4 - 0s - loss: 0.6273 - accuracy: 0.8500 - val_loss: 0.6820 - val_accuracy: 0.7000\n",
            "Epoch 34/200\n",
            "4/4 - 0s - loss: 0.6154 - accuracy: 0.8500 - val_loss: 0.6680 - val_accuracy: 0.7000\n",
            "Epoch 35/200\n",
            "4/4 - 0s - loss: 0.6029 - accuracy: 0.8417 - val_loss: 0.6589 - val_accuracy: 0.7000\n",
            "Epoch 36/200\n",
            "4/4 - 0s - loss: 0.5905 - accuracy: 0.8417 - val_loss: 0.6471 - val_accuracy: 0.7000\n",
            "Epoch 37/200\n",
            "4/4 - 0s - loss: 0.5793 - accuracy: 0.8417 - val_loss: 0.6327 - val_accuracy: 0.7000\n",
            "Epoch 38/200\n",
            "4/4 - 0s - loss: 0.5671 - accuracy: 0.8583 - val_loss: 0.6179 - val_accuracy: 0.7000\n",
            "Epoch 39/200\n",
            "4/4 - 0s - loss: 0.5562 - accuracy: 0.8750 - val_loss: 0.6062 - val_accuracy: 0.7000\n",
            "Epoch 40/200\n",
            "4/4 - 0s - loss: 0.5450 - accuracy: 0.8750 - val_loss: 0.5941 - val_accuracy: 0.7333\n",
            "Epoch 41/200\n",
            "4/4 - 0s - loss: 0.5341 - accuracy: 0.8750 - val_loss: 0.5870 - val_accuracy: 0.7000\n",
            "Epoch 42/200\n",
            "4/4 - 0s - loss: 0.5246 - accuracy: 0.8667 - val_loss: 0.5797 - val_accuracy: 0.7000\n",
            "Epoch 43/200\n",
            "4/4 - 0s - loss: 0.5140 - accuracy: 0.8667 - val_loss: 0.5674 - val_accuracy: 0.7333\n",
            "Epoch 44/200\n",
            "4/4 - 0s - loss: 0.5044 - accuracy: 0.8750 - val_loss: 0.5574 - val_accuracy: 0.7333\n",
            "Epoch 45/200\n",
            "4/4 - 0s - loss: 0.4954 - accuracy: 0.8750 - val_loss: 0.5458 - val_accuracy: 0.7333\n",
            "Epoch 46/200\n",
            "4/4 - 0s - loss: 0.4863 - accuracy: 0.8833 - val_loss: 0.5373 - val_accuracy: 0.7333\n",
            "Epoch 47/200\n",
            "4/4 - 0s - loss: 0.4778 - accuracy: 0.8833 - val_loss: 0.5292 - val_accuracy: 0.7667\n",
            "Epoch 48/200\n",
            "4/4 - 0s - loss: 0.4695 - accuracy: 0.8833 - val_loss: 0.5195 - val_accuracy: 0.8000\n",
            "Epoch 49/200\n",
            "4/4 - 0s - loss: 0.4615 - accuracy: 0.8917 - val_loss: 0.5124 - val_accuracy: 0.7667\n",
            "Epoch 50/200\n",
            "4/4 - 0s - loss: 0.4538 - accuracy: 0.8833 - val_loss: 0.5049 - val_accuracy: 0.8000\n",
            "Epoch 51/200\n",
            "4/4 - 0s - loss: 0.4459 - accuracy: 0.8917 - val_loss: 0.4951 - val_accuracy: 0.8333\n",
            "Epoch 52/200\n",
            "4/4 - 0s - loss: 0.4395 - accuracy: 0.9333 - val_loss: 0.4810 - val_accuracy: 0.8333\n",
            "Epoch 53/200\n",
            "4/4 - 0s - loss: 0.4309 - accuracy: 0.9417 - val_loss: 0.4702 - val_accuracy: 0.8333\n",
            "Epoch 54/200\n",
            "4/4 - 0s - loss: 0.4208 - accuracy: 0.9417 - val_loss: 0.4610 - val_accuracy: 0.8333\n",
            "Epoch 55/200\n",
            "4/4 - 0s - loss: 0.4104 - accuracy: 0.9417 - val_loss: 0.4532 - val_accuracy: 0.8333\n",
            "Epoch 56/200\n",
            "4/4 - 0s - loss: 0.3991 - accuracy: 0.9250 - val_loss: 0.4450 - val_accuracy: 0.8333\n",
            "Epoch 57/200\n",
            "4/4 - 0s - loss: 0.3896 - accuracy: 0.9333 - val_loss: 0.4361 - val_accuracy: 0.8333\n",
            "Epoch 58/200\n",
            "4/4 - 0s - loss: 0.3804 - accuracy: 0.9333 - val_loss: 0.4272 - val_accuracy: 0.8333\n",
            "Epoch 59/200\n",
            "4/4 - 0s - loss: 0.3724 - accuracy: 0.9500 - val_loss: 0.4185 - val_accuracy: 0.8333\n",
            "Epoch 60/200\n",
            "4/4 - 0s - loss: 0.3665 - accuracy: 0.9500 - val_loss: 0.4099 - val_accuracy: 0.8333\n",
            "Epoch 61/200\n",
            "4/4 - 0s - loss: 0.3607 - accuracy: 0.9500 - val_loss: 0.4021 - val_accuracy: 0.8333\n",
            "Epoch 62/200\n",
            "4/4 - 0s - loss: 0.3542 - accuracy: 0.9500 - val_loss: 0.4014 - val_accuracy: 0.8333\n",
            "Epoch 63/200\n",
            "4/4 - 0s - loss: 0.3487 - accuracy: 0.9500 - val_loss: 0.3988 - val_accuracy: 0.8333\n",
            "Epoch 64/200\n",
            "4/4 - 0s - loss: 0.3431 - accuracy: 0.9500 - val_loss: 0.3896 - val_accuracy: 0.8333\n",
            "Epoch 65/200\n",
            "4/4 - 0s - loss: 0.3377 - accuracy: 0.9500 - val_loss: 0.3816 - val_accuracy: 0.8667\n",
            "Epoch 66/200\n",
            "4/4 - 0s - loss: 0.3327 - accuracy: 0.9500 - val_loss: 0.3761 - val_accuracy: 0.9000\n",
            "Epoch 67/200\n",
            "4/4 - 0s - loss: 0.3280 - accuracy: 0.9500 - val_loss: 0.3721 - val_accuracy: 0.9000\n",
            "Epoch 68/200\n",
            "4/4 - 0s - loss: 0.3237 - accuracy: 0.9583 - val_loss: 0.3659 - val_accuracy: 0.9000\n",
            "Epoch 69/200\n",
            "4/4 - 0s - loss: 0.3190 - accuracy: 0.9667 - val_loss: 0.3639 - val_accuracy: 0.9000\n",
            "Epoch 70/200\n",
            "4/4 - 0s - loss: 0.3147 - accuracy: 0.9500 - val_loss: 0.3647 - val_accuracy: 0.8667\n",
            "Epoch 71/200\n",
            "4/4 - 0s - loss: 0.3100 - accuracy: 0.9500 - val_loss: 0.3592 - val_accuracy: 0.9000\n",
            "Epoch 72/200\n",
            "4/4 - 0s - loss: 0.3063 - accuracy: 0.9667 - val_loss: 0.3508 - val_accuracy: 0.9000\n",
            "Epoch 73/200\n",
            "4/4 - 0s - loss: 0.3013 - accuracy: 0.9667 - val_loss: 0.3473 - val_accuracy: 0.9000\n",
            "Epoch 74/200\n",
            "4/4 - 0s - loss: 0.2991 - accuracy: 0.9667 - val_loss: 0.3479 - val_accuracy: 0.9000\n",
            "Epoch 75/200\n",
            "4/4 - 0s - loss: 0.2931 - accuracy: 0.9667 - val_loss: 0.3382 - val_accuracy: 0.9000\n",
            "Epoch 76/200\n",
            "4/4 - 0s - loss: 0.2890 - accuracy: 0.9667 - val_loss: 0.3291 - val_accuracy: 0.9000\n",
            "Epoch 77/200\n",
            "4/4 - 0s - loss: 0.2853 - accuracy: 0.9667 - val_loss: 0.3244 - val_accuracy: 0.9000\n",
            "Epoch 78/200\n",
            "4/4 - 0s - loss: 0.2820 - accuracy: 0.9667 - val_loss: 0.3258 - val_accuracy: 0.9000\n",
            "Epoch 79/200\n",
            "4/4 - 0s - loss: 0.2769 - accuracy: 0.9667 - val_loss: 0.3220 - val_accuracy: 0.9000\n",
            "Epoch 80/200\n",
            "4/4 - 0s - loss: 0.2731 - accuracy: 0.9667 - val_loss: 0.3178 - val_accuracy: 0.9000\n",
            "Epoch 81/200\n",
            "4/4 - 0s - loss: 0.2709 - accuracy: 0.9667 - val_loss: 0.3101 - val_accuracy: 0.9000\n",
            "Epoch 82/200\n",
            "4/4 - 0s - loss: 0.2650 - accuracy: 0.9667 - val_loss: 0.3112 - val_accuracy: 0.9000\n",
            "Epoch 83/200\n",
            "4/4 - 0s - loss: 0.2618 - accuracy: 0.9667 - val_loss: 0.3106 - val_accuracy: 0.9000\n",
            "Epoch 84/200\n",
            "4/4 - 0s - loss: 0.2579 - accuracy: 0.9667 - val_loss: 0.3030 - val_accuracy: 0.9000\n",
            "Epoch 85/200\n",
            "4/4 - 0s - loss: 0.2539 - accuracy: 0.9667 - val_loss: 0.2966 - val_accuracy: 0.9333\n",
            "Epoch 86/200\n",
            "4/4 - 0s - loss: 0.2511 - accuracy: 0.9750 - val_loss: 0.2887 - val_accuracy: 0.9667\n",
            "Epoch 87/200\n",
            "4/4 - 0s - loss: 0.2466 - accuracy: 0.9750 - val_loss: 0.2878 - val_accuracy: 0.9667\n",
            "Epoch 88/200\n",
            "4/4 - 0s - loss: 0.2431 - accuracy: 0.9667 - val_loss: 0.2904 - val_accuracy: 0.9000\n",
            "Epoch 89/200\n",
            "4/4 - 0s - loss: 0.2396 - accuracy: 0.9667 - val_loss: 0.2853 - val_accuracy: 0.9333\n",
            "Epoch 90/200\n",
            "4/4 - 0s - loss: 0.2375 - accuracy: 0.9667 - val_loss: 0.2762 - val_accuracy: 0.9667\n",
            "Epoch 91/200\n",
            "4/4 - 0s - loss: 0.2323 - accuracy: 0.9750 - val_loss: 0.2775 - val_accuracy: 0.9333\n",
            "Epoch 92/200\n",
            "4/4 - 0s - loss: 0.2290 - accuracy: 0.9667 - val_loss: 0.2757 - val_accuracy: 0.9333\n",
            "Epoch 93/200\n",
            "4/4 - 0s - loss: 0.2256 - accuracy: 0.9667 - val_loss: 0.2696 - val_accuracy: 0.9667\n",
            "Epoch 94/200\n",
            "4/4 - 0s - loss: 0.2229 - accuracy: 0.9750 - val_loss: 0.2595 - val_accuracy: 0.9667\n",
            "Epoch 95/200\n",
            "4/4 - 0s - loss: 0.2193 - accuracy: 0.9750 - val_loss: 0.2576 - val_accuracy: 0.9667\n",
            "Epoch 96/200\n",
            "4/4 - 0s - loss: 0.2176 - accuracy: 0.9667 - val_loss: 0.2631 - val_accuracy: 0.9333\n",
            "Epoch 97/200\n",
            "4/4 - 0s - loss: 0.2128 - accuracy: 0.9667 - val_loss: 0.2560 - val_accuracy: 0.9667\n",
            "Epoch 98/200\n",
            "4/4 - 0s - loss: 0.2093 - accuracy: 0.9667 - val_loss: 0.2446 - val_accuracy: 1.0000\n",
            "Epoch 99/200\n",
            "4/4 - 0s - loss: 0.2069 - accuracy: 0.9750 - val_loss: 0.2414 - val_accuracy: 1.0000\n",
            "Epoch 100/200\n",
            "4/4 - 0s - loss: 0.2037 - accuracy: 0.9750 - val_loss: 0.2431 - val_accuracy: 0.9667\n",
            "Epoch 101/200\n",
            "4/4 - 0s - loss: 0.2021 - accuracy: 0.9750 - val_loss: 0.2467 - val_accuracy: 0.9667\n",
            "Epoch 102/200\n",
            "4/4 - 0s - loss: 0.1988 - accuracy: 0.9667 - val_loss: 0.2342 - val_accuracy: 1.0000\n",
            "Epoch 103/200\n",
            "4/4 - 0s - loss: 0.1955 - accuracy: 0.9750 - val_loss: 0.2303 - val_accuracy: 1.0000\n",
            "Epoch 104/200\n",
            "4/4 - 0s - loss: 0.1930 - accuracy: 0.9750 - val_loss: 0.2332 - val_accuracy: 0.9667\n",
            "Epoch 105/200\n",
            "4/4 - 0s - loss: 0.1892 - accuracy: 0.9750 - val_loss: 0.2306 - val_accuracy: 0.9667\n",
            "Epoch 106/200\n",
            "4/4 - 0s - loss: 0.1868 - accuracy: 0.9750 - val_loss: 0.2264 - val_accuracy: 0.9667\n",
            "Epoch 107/200\n",
            "4/4 - 0s - loss: 0.1845 - accuracy: 0.9750 - val_loss: 0.2227 - val_accuracy: 0.9667\n",
            "Epoch 108/200\n",
            "4/4 - 0s - loss: 0.1815 - accuracy: 0.9750 - val_loss: 0.2183 - val_accuracy: 1.0000\n",
            "Epoch 109/200\n",
            "4/4 - 0s - loss: 0.1796 - accuracy: 0.9750 - val_loss: 0.2170 - val_accuracy: 1.0000\n",
            "Epoch 110/200\n",
            "4/4 - 0s - loss: 0.1773 - accuracy: 0.9750 - val_loss: 0.2144 - val_accuracy: 1.0000\n",
            "Epoch 111/200\n",
            "4/4 - 0s - loss: 0.1744 - accuracy: 0.9750 - val_loss: 0.2102 - val_accuracy: 1.0000\n",
            "Epoch 112/200\n",
            "4/4 - 0s - loss: 0.1724 - accuracy: 0.9750 - val_loss: 0.2058 - val_accuracy: 1.0000\n",
            "Epoch 113/200\n",
            "4/4 - 0s - loss: 0.1701 - accuracy: 0.9750 - val_loss: 0.2041 - val_accuracy: 1.0000\n",
            "Epoch 114/200\n",
            "4/4 - 0s - loss: 0.1675 - accuracy: 0.9750 - val_loss: 0.2044 - val_accuracy: 1.0000\n",
            "Epoch 115/200\n",
            "4/4 - 0s - loss: 0.1655 - accuracy: 0.9750 - val_loss: 0.2043 - val_accuracy: 0.9667\n",
            "Epoch 116/200\n",
            "4/4 - 0s - loss: 0.1633 - accuracy: 0.9750 - val_loss: 0.1990 - val_accuracy: 1.0000\n",
            "Epoch 117/200\n",
            "4/4 - 0s - loss: 0.1608 - accuracy: 0.9750 - val_loss: 0.1957 - val_accuracy: 1.0000\n",
            "Epoch 118/200\n",
            "4/4 - 0s - loss: 0.1574 - accuracy: 0.9750 - val_loss: 0.1993 - val_accuracy: 0.9667\n",
            "Epoch 119/200\n",
            "4/4 - 0s - loss: 0.1570 - accuracy: 0.9750 - val_loss: 0.1990 - val_accuracy: 0.9667\n",
            "Epoch 120/200\n",
            "4/4 - 0s - loss: 0.1546 - accuracy: 0.9667 - val_loss: 0.1819 - val_accuracy: 1.0000\n",
            "Epoch 121/200\n",
            "4/4 - 0s - loss: 0.1515 - accuracy: 0.9667 - val_loss: 0.1849 - val_accuracy: 1.0000\n",
            "Epoch 122/200\n",
            "4/4 - 0s - loss: 0.1485 - accuracy: 0.9750 - val_loss: 0.1865 - val_accuracy: 0.9667\n",
            "Epoch 123/200\n",
            "4/4 - 0s - loss: 0.1464 - accuracy: 0.9750 - val_loss: 0.1832 - val_accuracy: 0.9667\n",
            "Epoch 124/200\n",
            "4/4 - 0s - loss: 0.1447 - accuracy: 0.9750 - val_loss: 0.1789 - val_accuracy: 1.0000\n",
            "Epoch 125/200\n",
            "4/4 - 0s - loss: 0.1425 - accuracy: 0.9750 - val_loss: 0.1819 - val_accuracy: 0.9667\n",
            "Epoch 126/200\n",
            "4/4 - 0s - loss: 0.1405 - accuracy: 0.9750 - val_loss: 0.1770 - val_accuracy: 0.9667\n",
            "Epoch 127/200\n",
            "4/4 - 0s - loss: 0.1385 - accuracy: 0.9750 - val_loss: 0.1732 - val_accuracy: 1.0000\n",
            "Epoch 128/200\n",
            "4/4 - 0s - loss: 0.1364 - accuracy: 0.9750 - val_loss: 0.1707 - val_accuracy: 1.0000\n",
            "Epoch 129/200\n",
            "4/4 - 0s - loss: 0.1348 - accuracy: 0.9750 - val_loss: 0.1715 - val_accuracy: 0.9667\n",
            "Epoch 130/200\n",
            "4/4 - 0s - loss: 0.1333 - accuracy: 0.9750 - val_loss: 0.1652 - val_accuracy: 1.0000\n",
            "Epoch 131/200\n",
            "4/4 - 0s - loss: 0.1315 - accuracy: 0.9667 - val_loss: 0.1634 - val_accuracy: 1.0000\n",
            "Epoch 132/200\n",
            "4/4 - 0s - loss: 0.1311 - accuracy: 0.9750 - val_loss: 0.1702 - val_accuracy: 0.9667\n",
            "Epoch 133/200\n",
            "4/4 - 0s - loss: 0.1289 - accuracy: 0.9750 - val_loss: 0.1617 - val_accuracy: 1.0000\n",
            "Epoch 134/200\n",
            "4/4 - 0s - loss: 0.1272 - accuracy: 0.9750 - val_loss: 0.1526 - val_accuracy: 1.0000\n",
            "Epoch 135/200\n",
            "4/4 - 0s - loss: 0.1258 - accuracy: 0.9667 - val_loss: 0.1526 - val_accuracy: 1.0000\n",
            "Epoch 136/200\n",
            "4/4 - 0s - loss: 0.1234 - accuracy: 0.9833 - val_loss: 0.1577 - val_accuracy: 1.0000\n",
            "Epoch 137/200\n",
            "4/4 - 0s - loss: 0.1234 - accuracy: 0.9833 - val_loss: 0.1547 - val_accuracy: 1.0000\n",
            "Epoch 138/200\n",
            "4/4 - 0s - loss: 0.1211 - accuracy: 0.9750 - val_loss: 0.1576 - val_accuracy: 0.9667\n",
            "Epoch 139/200\n",
            "4/4 - 0s - loss: 0.1198 - accuracy: 0.9750 - val_loss: 0.1510 - val_accuracy: 1.0000\n",
            "Epoch 140/200\n",
            "4/4 - 0s - loss: 0.1183 - accuracy: 0.9750 - val_loss: 0.1452 - val_accuracy: 1.0000\n",
            "Epoch 141/200\n",
            "4/4 - 0s - loss: 0.1178 - accuracy: 0.9750 - val_loss: 0.1473 - val_accuracy: 1.0000\n",
            "Epoch 142/200\n",
            "4/4 - 0s - loss: 0.1163 - accuracy: 0.9750 - val_loss: 0.1442 - val_accuracy: 1.0000\n",
            "Epoch 143/200\n",
            "4/4 - 0s - loss: 0.1145 - accuracy: 0.9750 - val_loss: 0.1416 - val_accuracy: 1.0000\n",
            "Epoch 144/200\n",
            "4/4 - 0s - loss: 0.1132 - accuracy: 0.9750 - val_loss: 0.1377 - val_accuracy: 1.0000\n",
            "Epoch 145/200\n",
            "4/4 - 0s - loss: 0.1139 - accuracy: 0.9667 - val_loss: 0.1333 - val_accuracy: 1.0000\n",
            "Epoch 146/200\n",
            "4/4 - 0s - loss: 0.1116 - accuracy: 0.9750 - val_loss: 0.1361 - val_accuracy: 1.0000\n",
            "Epoch 147/200\n",
            "4/4 - 0s - loss: 0.1100 - accuracy: 0.9750 - val_loss: 0.1462 - val_accuracy: 0.9667\n",
            "Epoch 148/200\n",
            "4/4 - 0s - loss: 0.1117 - accuracy: 0.9750 - val_loss: 0.1444 - val_accuracy: 0.9667\n",
            "Epoch 149/200\n",
            "4/4 - 0s - loss: 0.1087 - accuracy: 0.9833 - val_loss: 0.1320 - val_accuracy: 1.0000\n",
            "Epoch 150/200\n",
            "4/4 - 0s - loss: 0.1072 - accuracy: 0.9750 - val_loss: 0.1287 - val_accuracy: 1.0000\n",
            "Epoch 151/200\n",
            "4/4 - 0s - loss: 0.1078 - accuracy: 0.9667 - val_loss: 0.1257 - val_accuracy: 1.0000\n",
            "Epoch 152/200\n",
            "4/4 - 0s - loss: 0.1077 - accuracy: 0.9667 - val_loss: 0.1339 - val_accuracy: 1.0000\n",
            "Epoch 153/200\n",
            "4/4 - 0s - loss: 0.1050 - accuracy: 0.9833 - val_loss: 0.1295 - val_accuracy: 1.0000\n",
            "Epoch 154/200\n",
            "4/4 - 0s - loss: 0.1043 - accuracy: 0.9750 - val_loss: 0.1295 - val_accuracy: 1.0000\n",
            "Epoch 155/200\n",
            "4/4 - 0s - loss: 0.1031 - accuracy: 0.9750 - val_loss: 0.1244 - val_accuracy: 1.0000\n",
            "Epoch 156/200\n",
            "4/4 - 0s - loss: 0.1029 - accuracy: 0.9750 - val_loss: 0.1256 - val_accuracy: 1.0000\n",
            "Epoch 157/200\n",
            "4/4 - 0s - loss: 0.1021 - accuracy: 0.9750 - val_loss: 0.1232 - val_accuracy: 1.0000\n",
            "Epoch 158/200\n",
            "4/4 - 0s - loss: 0.1009 - accuracy: 0.9750 - val_loss: 0.1183 - val_accuracy: 1.0000\n",
            "Epoch 159/200\n",
            "4/4 - 0s - loss: 0.1005 - accuracy: 0.9750 - val_loss: 0.1205 - val_accuracy: 1.0000\n",
            "Epoch 160/200\n",
            "4/4 - 0s - loss: 0.1001 - accuracy: 0.9750 - val_loss: 0.1183 - val_accuracy: 1.0000\n",
            "Epoch 161/200\n",
            "4/4 - 0s - loss: 0.0993 - accuracy: 0.9667 - val_loss: 0.1256 - val_accuracy: 1.0000\n",
            "Epoch 162/200\n",
            "4/4 - 0s - loss: 0.0985 - accuracy: 0.9750 - val_loss: 0.1187 - val_accuracy: 1.0000\n",
            "Epoch 163/200\n",
            "4/4 - 0s - loss: 0.0974 - accuracy: 0.9750 - val_loss: 0.1160 - val_accuracy: 1.0000\n",
            "Epoch 164/200\n",
            "4/4 - 0s - loss: 0.0967 - accuracy: 0.9750 - val_loss: 0.1163 - val_accuracy: 1.0000\n",
            "Epoch 165/200\n",
            "4/4 - 0s - loss: 0.0962 - accuracy: 0.9750 - val_loss: 0.1139 - val_accuracy: 1.0000\n",
            "Epoch 166/200\n",
            "4/4 - 0s - loss: 0.0974 - accuracy: 0.9667 - val_loss: 0.1172 - val_accuracy: 1.0000\n",
            "Epoch 167/200\n",
            "4/4 - 0s - loss: 0.0951 - accuracy: 0.9750 - val_loss: 0.1110 - val_accuracy: 1.0000\n",
            "Epoch 168/200\n",
            "4/4 - 0s - loss: 0.0968 - accuracy: 0.9667 - val_loss: 0.1077 - val_accuracy: 1.0000\n",
            "Epoch 169/200\n",
            "4/4 - 0s - loss: 0.0930 - accuracy: 0.9750 - val_loss: 0.1163 - val_accuracy: 1.0000\n",
            "Epoch 170/200\n",
            "4/4 - 0s - loss: 0.0960 - accuracy: 0.9750 - val_loss: 0.1281 - val_accuracy: 0.9667\n",
            "Epoch 171/200\n",
            "4/4 - 0s - loss: 0.0948 - accuracy: 0.9750 - val_loss: 0.1117 - val_accuracy: 1.0000\n",
            "Epoch 172/200\n",
            "4/4 - 0s - loss: 0.0923 - accuracy: 0.9667 - val_loss: 0.1070 - val_accuracy: 1.0000\n",
            "Epoch 173/200\n",
            "4/4 - 0s - loss: 0.0961 - accuracy: 0.9750 - val_loss: 0.1001 - val_accuracy: 1.0000\n",
            "Epoch 174/200\n",
            "4/4 - 0s - loss: 0.0919 - accuracy: 0.9667 - val_loss: 0.1056 - val_accuracy: 1.0000\n",
            "Epoch 175/200\n",
            "4/4 - 0s - loss: 0.0892 - accuracy: 0.9833 - val_loss: 0.1166 - val_accuracy: 0.9667\n",
            "Epoch 176/200\n",
            "4/4 - 0s - loss: 0.0986 - accuracy: 0.9750 - val_loss: 0.1300 - val_accuracy: 0.9667\n",
            "Epoch 177/200\n",
            "4/4 - 0s - loss: 0.0959 - accuracy: 0.9750 - val_loss: 0.1041 - val_accuracy: 1.0000\n",
            "Epoch 178/200\n",
            "4/4 - 0s - loss: 0.0893 - accuracy: 0.9750 - val_loss: 0.1013 - val_accuracy: 1.0000\n",
            "Epoch 179/200\n",
            "4/4 - 0s - loss: 0.0912 - accuracy: 0.9667 - val_loss: 0.0968 - val_accuracy: 1.0000\n",
            "Epoch 180/200\n",
            "4/4 - 0s - loss: 0.0901 - accuracy: 0.9667 - val_loss: 0.1029 - val_accuracy: 1.0000\n",
            "Epoch 181/200\n",
            "4/4 - 0s - loss: 0.0914 - accuracy: 0.9667 - val_loss: 0.1113 - val_accuracy: 1.0000\n",
            "Epoch 182/200\n",
            "4/4 - 0s - loss: 0.0880 - accuracy: 0.9750 - val_loss: 0.1029 - val_accuracy: 1.0000\n",
            "Epoch 183/200\n",
            "4/4 - 0s - loss: 0.0869 - accuracy: 0.9750 - val_loss: 0.0963 - val_accuracy: 1.0000\n",
            "Epoch 184/200\n",
            "4/4 - 0s - loss: 0.0877 - accuracy: 0.9667 - val_loss: 0.0947 - val_accuracy: 1.0000\n",
            "Epoch 185/200\n",
            "4/4 - 0s - loss: 0.0871 - accuracy: 0.9667 - val_loss: 0.0970 - val_accuracy: 1.0000\n",
            "Epoch 186/200\n",
            "4/4 - 0s - loss: 0.0874 - accuracy: 0.9833 - val_loss: 0.1094 - val_accuracy: 1.0000\n",
            "Epoch 187/200\n",
            "4/4 - 0s - loss: 0.0872 - accuracy: 0.9750 - val_loss: 0.1055 - val_accuracy: 1.0000\n",
            "Epoch 188/200\n",
            "4/4 - 0s - loss: 0.0851 - accuracy: 0.9750 - val_loss: 0.0959 - val_accuracy: 1.0000\n",
            "Epoch 189/200\n",
            "4/4 - 0s - loss: 0.0845 - accuracy: 0.9750 - val_loss: 0.0910 - val_accuracy: 1.0000\n",
            "Epoch 190/200\n",
            "4/4 - 0s - loss: 0.0882 - accuracy: 0.9667 - val_loss: 0.0897 - val_accuracy: 1.0000\n",
            "Epoch 191/200\n",
            "4/4 - 0s - loss: 0.0846 - accuracy: 0.9667 - val_loss: 0.0953 - val_accuracy: 1.0000\n",
            "Epoch 192/200\n",
            "4/4 - 0s - loss: 0.0841 - accuracy: 0.9750 - val_loss: 0.1043 - val_accuracy: 1.0000\n",
            "Epoch 193/200\n",
            "4/4 - 0s - loss: 0.0874 - accuracy: 0.9750 - val_loss: 0.1063 - val_accuracy: 1.0000\n",
            "Epoch 194/200\n",
            "4/4 - 0s - loss: 0.0828 - accuracy: 0.9750 - val_loss: 0.0916 - val_accuracy: 1.0000\n",
            "Epoch 195/200\n",
            "4/4 - 0s - loss: 0.0841 - accuracy: 0.9750 - val_loss: 0.0859 - val_accuracy: 1.0000\n",
            "Epoch 196/200\n",
            "4/4 - 0s - loss: 0.0866 - accuracy: 0.9667 - val_loss: 0.0863 - val_accuracy: 1.0000\n",
            "Epoch 197/200\n",
            "4/4 - 0s - loss: 0.0838 - accuracy: 0.9750 - val_loss: 0.0988 - val_accuracy: 1.0000\n",
            "Epoch 198/200\n",
            "4/4 - 0s - loss: 0.0833 - accuracy: 0.9750 - val_loss: 0.1022 - val_accuracy: 1.0000\n",
            "Epoch 199/200\n",
            "4/4 - 0s - loss: 0.0836 - accuracy: 0.9750 - val_loss: 0.0943 - val_accuracy: 1.0000\n",
            "Epoch 200/200\n",
            "4/4 - 0s - loss: 0.0818 - accuracy: 0.9833 - val_loss: 0.0873 - val_accuracy: 1.0000\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_3 (Dense)              (None, 16)                80        \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 3)                 51        \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 3)                 0         \n",
            "=================================================================\n",
            "Total params: 403\n",
            "Trainable params: 403\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38DoamZ9LmLf",
        "outputId": "68d5780f-0939-4650-9252-21260fc2e335"
      },
      "source": [
        "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)\n",
        "\n",
        "print('\\nAcuácia de Teste', test_acc)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/1 - 0s - loss: 0.0873 - accuracy: 1.0000\n",
            "\n",
            "Acuácia de Teste 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "if-ZbJt2Lw_o",
        "outputId": "42b856dc-7dd2-4461-b83c-c744ea53bce9"
      },
      "source": [
        "predictions = model.predict(X_test)\n",
        "\n",
        "class_names = ['setosa','versicolor','virginica']\n",
        "\n",
        "for teste in predictions:\n",
        "  print(f'''\n",
        "% de ser {class_names[0]} = {teste[0]*100}\n",
        "% de ser {class_names[1]} = {teste[1]*100}\n",
        "% de ser {class_names[2]} = {teste[2]*100}\n",
        "        ''' )"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "% de ser setosa = 99.88346695899963\n",
            "% de ser versicolor = 0.11653434485197067\n",
            "% de ser virginica = 1.3089761219386986e-08\n",
            "        \n",
            "\n",
            "% de ser setosa = 6.959909945726395\n",
            "% de ser versicolor = 92.81322956085205\n",
            "% de ser virginica = 0.22686063311994076\n",
            "        \n",
            "\n",
            "% de ser setosa = 0.3024816047400236\n",
            "% de ser versicolor = 98.76106977462769\n",
            "% de ser virginica = 0.936457421630621\n",
            "        \n",
            "\n",
            "% de ser setosa = 99.85038042068481\n",
            "% de ser versicolor = 0.14962145360186696\n",
            "% de ser virginica = 4.286357380145489e-08\n",
            "        \n",
            "\n",
            "% de ser setosa = 0.0011134449778182898\n",
            "% de ser versicolor = 28.64833176136017\n",
            "% de ser virginica = 71.35055661201477\n",
            "        \n",
            "\n",
            "% de ser setosa = 0.137752341106534\n",
            "% de ser versicolor = 90.30100703239441\n",
            "% de ser virginica = 9.561233222484589\n",
            "        \n",
            "\n",
            "% de ser setosa = 0.002537761611165479\n",
            "% de ser versicolor = 8.802079409360886\n",
            "% de ser virginica = 91.19538068771362\n",
            "        \n",
            "\n",
            "% de ser setosa = 99.66506361961365\n",
            "% de ser versicolor = 0.3349324921146035\n",
            "% de ser virginica = 2.8469131496677846e-07\n",
            "        \n",
            "\n",
            "% de ser setosa = 99.47271943092346\n",
            "% de ser versicolor = 0.5272763781249523\n",
            "% de ser virginica = 9.335402673116278e-07\n",
            "        \n",
            "\n",
            "% de ser setosa = 9.155987754638772e-05\n",
            "% de ser versicolor = 1.3240205124020576\n",
            "% de ser virginica = 98.67588877677917\n",
            "        \n",
            "\n",
            "% de ser setosa = 0.2747193444520235\n",
            "% de ser versicolor = 92.62176752090454\n",
            "% de ser virginica = 7.103509455919266\n",
            "        \n",
            "\n",
            "% de ser setosa = 99.80862140655518\n",
            "% de ser versicolor = 0.1913868822157383\n",
            "% de ser virginica = 6.09982453436686e-08\n",
            "        \n",
            "\n",
            "% de ser setosa = 9.378792924508161e-05\n",
            "% de ser versicolor = 2.245236001908779\n",
            "% de ser virginica = 97.7546751499176\n",
            "        \n",
            "\n",
            "% de ser setosa = 0.24858927354216576\n",
            "% de ser versicolor = 97.1527636051178\n",
            "% de ser virginica = 2.5986431166529655\n",
            "        \n",
            "\n",
            "% de ser setosa = 0.13985579134896398\n",
            "% de ser versicolor = 85.85183620452881\n",
            "% de ser virginica = 14.008305966854095\n",
            "        \n",
            "\n",
            "% de ser setosa = 99.58637952804565\n",
            "% de ser versicolor = 0.4136135336011648\n",
            "% de ser virginica = 1.033355978563577e-06\n",
            "        \n",
            "\n",
            "% de ser setosa = 0.5025509744882584\n",
            "% de ser versicolor = 98.04915189743042\n",
            "% de ser virginica = 1.4482884667813778\n",
            "        \n",
            "\n",
            "% de ser setosa = 0.116964359767735\n",
            "% de ser versicolor = 72.40474224090576\n",
            "% de ser virginica = 27.47828960418701\n",
            "        \n",
            "\n",
            "% de ser setosa = 99.59737658500671\n",
            "% de ser versicolor = 0.40262509137392044\n",
            "% de ser virginica = 2.448777181029982e-07\n",
            "        \n",
            "\n",
            "% de ser setosa = 99.59664344787598\n",
            "% de ser versicolor = 0.4033525474369526\n",
            "% de ser virginica = 4.6867434377872996e-07\n",
            "        \n",
            "\n",
            "% de ser setosa = 0.14645465416833758\n",
            "% de ser versicolor = 87.14258074760437\n",
            "% de ser virginica = 12.710969150066376\n",
            "        \n",
            "\n",
            "% de ser setosa = 0.09199335472658277\n",
            "% de ser versicolor = 60.44333577156067\n",
            "% de ser virginica = 39.46467638015747\n",
            "        \n",
            "\n",
            "% de ser setosa = 0.037190577131696045\n",
            "% de ser versicolor = 70.91938257217407\n",
            "% de ser virginica = 29.043424129486084\n",
            "        \n",
            "\n",
            "% de ser setosa = 99.73076581954956\n",
            "% de ser versicolor = 0.2692373702302575\n",
            "% de ser virginica = 2.1716395348647666e-07\n",
            "        \n",
            "\n",
            "% de ser setosa = 0.0009272514944314025\n",
            "% de ser versicolor = 17.269977927207947\n",
            "% de ser virginica = 82.7290952205658\n",
            "        \n",
            "\n",
            "% de ser setosa = 0.4257498774677515\n",
            "% de ser versicolor = 96.50841355323792\n",
            "% de ser virginica = 3.0658258125185966\n",
            "        \n",
            "\n",
            "% de ser setosa = 99.90969896316528\n",
            "% de ser versicolor = 0.09030694491229951\n",
            "% de ser virginica = 1.058763354766512e-08\n",
            "        \n",
            "\n",
            "% de ser setosa = 99.82088804244995\n",
            "% de ser versicolor = 0.17911598552018404\n",
            "% de ser virginica = 7.643009003466261e-08\n",
            "        \n",
            "\n",
            "% de ser setosa = 0.11018753284588456\n",
            "% de ser versicolor = 94.81586217880249\n",
            "% de ser virginica = 5.073942989110947\n",
            "        \n",
            "\n",
            "% de ser setosa = 0.0015407656974275596\n",
            "% de ser versicolor = 6.8317994475364685\n",
            "% de ser virginica = 93.1666612625122\n",
            "        \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}